<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>blog on Homepage Zhuo Liu</title><link>https://joeliuz6.github.io/tags/blog/</link><description>Recent content in blog on Homepage Zhuo Liu</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://joeliuz6.github.io/tags/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Towards Flexible Fairness-accuracy Tradeoff With Label Constraints</title><link>https://joeliuz6.github.io/en/1/01/01/fairness/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://joeliuz6.github.io/en/1/01/01/fairness/</guid><description>Introduction: What&amp;rsquo;s fairness? The concept &amp;ldquo;fairness&amp;rdquo; was first proposed in about late 2000s, attracting a wide interest together with ethical AI in the last decade. In general, fairness can be divided into three categories: group fairness, individual fairness and conterfactual fairness, where the former two are statistical dependence measures and the latter is rooted in causal inference. So how do we define fairness?
Values of fairness in real-world applications Fairness-Accuracy tradeoff Regularization and inference with label constraints Regularization with label constraints On-training approach: Inference-based training Learning plus inference Achieve flexible fairness-accuracy tradeoff at test time Learning with constraints YOTO YODO Inference with constraints References You Only Train Once: Loss-Conditional Training of Deep Networks Dataset Fairness: Achievable Fairness On Your Data With Utility Guarantees On Regularization and Inference with Label Constraints Men Also Like Shopping:Reducing Gender Bias Amplification using Corpus-level Constraints</description></item></channel></rss>